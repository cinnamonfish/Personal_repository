> 分类算法的评价方式

基尼系数


受试者工作曲线（ROC）
横坐标：TPR
纵坐标：FPR
绘制方法：所有样本按照LR值排序，然后设定一个阈值，前70%概率的样本点被记作1



> 知识
混淆矩阵
交叉验证
偏差方差权衡/博弈 bias variance tradeoff

## 分类算法
线性回归、逻辑回归、决策树、神经网络、关联分析、聚类、协同过滤、随机森林
	1. 线性可分情况
	2. 线性不可分情况
	**线性不可分**指的是数据之间有交叉，用包络线把样本点围起来那么包络线之间一定会交叉。
	
	### 逻辑斯蒂回归
	
	
	
	
	### 分类树/决策树
	
	
	### 随机森林
	
	
	KNN K最近邻法
	
	
	### 朴素贝叶斯Naïve Bayes classifier分类器
		一般指的是多项式贝叶斯，但是高斯贝叶斯也可以用（虽然一般不用
		原理：我们有一些元素和若干个分类，已经知道P(元素|分类)和P(分类)，用条件概率公式算出P(分类|元素)。
		朴素贝叶斯有一个问题，如果某一类某一项元素是0，那数据就永远不会被分到这一类。所以一般会给每个元素样本量加上1。
		朴素贝叶斯对语言的处理很简单，不考虑单词之间的顺序，所以有较大偏差bias（训练集方差会比较大），但同时方差是比较小的。
	高斯朴素贝叶斯分类
		假定元素的取值服从高斯分布
		和多项式朴素贝叶斯一样，不过假设我们已经知道了先验概率P(分类)和条件分布F(元素|分类) ，也就是均值和方差都知道。我们假设F(元素|分类)都是高斯分布。P(元素|分类)是从高斯分布算出来的p值。
		当然，这里面我们知道的信息就是这个人各项指标的数值。和多项式朴素贝叶斯不同的是，多项式情况里面的指标信息只有“0”和“1”，分布信息也只是0和1的个数，而高斯情况里面我们知道均值和标准差。
				
	
	线性判别分析 Linear discriminant analysis LDA 和 二次判别分析 QDA
		类似PCA，也是找到一个方向使得数据的分散程度最大。但LDA的目的是为了将数据分类；PCA只是希望找到离散程度最大的一个维度，对分类并不感兴趣（？）。
	
	 
	
		输入数据：一组观测，自变量X是n维连续/离散变量，因变量Y是k分类离散变量；同时需要知道Y不同取值下的X的条件分布
		目标：降维，寻找一个维度使得类内方差(scatter)最小、类间方差(distance)最大，并找到这个维度下的分界点。
		目标函数是Score function δ_k(x)。
		(1)降维
	
	
			多个维度：同样可以降为一维
			多个分类：distance的度量方式有点不太一样。
	
	最大间隔分类器
		首先，我们需要引入点到平面的距离公式，
		1/(|(|w ⃗ |)|  ) | w ⃗^T  〖\vec (x〗_(i )) +w_0 |  
		\frac{1} {||\vec{w}||}|\vec{w}^{T}\vec{x}_{i}+w_{0}|
		
		我们希望求出w = arg max (min 上式)（也就是最大间隔）
	支持向量分类器
		最大间隔分类器对异常值极其敏感，同时分割超平面有可能不存在。因此，使用软间隔 soft margin代替硬间隔对数据进行分类。
		支持向量分类器用到了交叉验证的方法。
		支持向量分类器的优化问题涉及到dot product，而不是观测本身。
	支持向量机SVM
		使用核函数对数据进行处理
		- 线性核函数
		K(xi, xj) = Σ(xi*xj)
		这种形式和使用支持向量分类器是一样的，因为支持向量分类器关于特征是线性的。从本质上来说，线性核函数是使用皮尔逊相关系数来度量变量之间的相关性。
		- 非线性核函数
		- 多项式核函数
		核函数形式为K(xi, xj) = (xi*xj+r)^d，这是自由度为d的多项式核函数。
		在算法中使用d>1的多项式核函数，能够生成光滑度更高的决策边界。
		- 径向核函数 Radial kernel / 径向基函数 Radial basal function
		其形式为 K(xi, xi') = exp(-γ · Σ(xij-xi'j)^2)        
		泰勒展开之后表示所有n维度上的两个观测的信息差异，用点积(dot product)的形式表示
	
		
		
		
		
参考资料：5种机器学习的分类器算法 - 知乎 (zhihu.com)
		youtube - StatQuest Machine Learning - YouTube
